# Data science

## Overview

Data science is the practice of extracting useful insights and predictions from data by combining statistics, programming, and domain knowledge. Typical workflows include framing a problem, collecting and preparing data, building and evaluating models, deploying them to production, and monitoring performance over time.

### Core components

- Problem definition and success metrics
- Data acquisition and exploratory data analysis
- Feature engineering and selection
- Model training, tuning, and evaluation
- Deployment, monitoring, and governance

---

## Oracle Cloud Infrastructure Data Science Professional

You’re completing the Oracle Cloud Infrastructure (OCI) Data Science Professional course. This focuses on building, operationalizing, and managing ML solutions on OCI.

### What you’ll practice

- Managed notebooks with OCI Data Science
- Conda environments and reproducibility
- Data flow and feature engineering at scale
- Training with Accelerated Data Science (ADS) SDK
- Experiment tracking, model catalogs, and model evaluation
- Model deployment with OCI Data Science Model Deployment
- MLOps on OCI: pipelines, monitoring, and governance

### Key skills and topics

- Supervised and unsupervised learning fundamentals
- Model selection, cross‑validation, and hyperparameter tuning
- Metrics: classification, regression, and ranking
- Handling imbalance, leakage, and drift
- Responsible AI: fairness, explainability, and privacy

---

## Quick study plan

- Week 1: Refresh Python, statistics, and EDA. Set up OCI tenancy and notebooks.
- Week 2: Build baseline models. Track experiments with ADS. Document results.
- Week 3: Feature engineering at scale. Tune models. Evaluate with robust metrics.
- Week 4: Package and deploy to OCI Model Deployment. Add monitoring and alerts.
- Week 5: End‑to‑end project: data to deployment with readme and handoff notes.

### Checklist

- [ ]  OCI tenancy configured and access keys stored securely
- [ ]  Notebook environment created with required Conda pack
- [ ]  Data source connected and documented
- [ ]  Baseline model and benchmark metric recorded
- [ ]  Tracked experiments with ADS or MLflow equivalent
- [ ]  Deployed model endpoint with versioning
- [ ]  Monitoring dashboards and drift checks in place
